{
  "terms": {
    "llm": {
      "term": "LLM",
      "full": "Large Language Model (Большая языковая модель)",
      "short": "ИИ-модель, обученная на огромных объёмах текста, понимающая и генерирующая человеческую речь",
      "long": "Большая языковая модель (LLM) — это тип искусственного интеллекта, обученный на массивных объёмах текстовых данных. Эти модели понимают контекст, отвечают на вопросы, пишут код и выполняют различные языковые задачи. Примеры: GPT-4, Claude, Llama.",
      "related": ["prompt", "token", "fine-tuning"],
      "example": "rlm = RLM.from_openai('gpt-4o')"
    },
    "rag": {
      "term": "RAG",
      "full": "Retrieval-Augmented Generation (Генерация с дополнением извлечением)",
      "short": "Техника, где LLM сначала ищет релевантные документы, а потом генерирует ответ",
      "long": "RAG объединяет мощь LLM с внешним поиском знаний. Вместо того чтобы полагаться только на данные обучения, модель сначала ищет в базе знаний релевантную информацию, затем использует этот контекст для генерации более точных и актуальных ответов.",
      "related": ["vector-store", "embedding", "retriever"],
      "example": "retriever = vectorstore.as_retriever()\nrlm.set_retriever(retriever)"
    },
    "embedding": {
      "term": "Эмбеддинг",
      "full": "Vector Embedding (Векторное представление)",
      "short": "Числовое представление текста, отражающее его смысл",
      "long": "Эмбеддинги преобразуют текст в векторы (списки чисел), где похожие значения находятся близко друг к другу в векторном пространстве. Это позволяет компьютерам понимать семантическую близость текстов, что нужно для поиска, кластеризации и рекомендаций.",
      "related": ["vector-store", "similarity-search", "rag"],
      "example": "embeddings = OpenAIEmbeddings()\nvector = embeddings.embed_query('Привет мир')"
    },
    "vector-store": {
      "term": "Векторное хранилище",
      "full": "Vector Database (Векторная база данных)",
      "short": "База данных, оптимизированная для хранения и поиска векторных эмбеддингов",
      "long": "Векторные хранилища — специализированные базы данных для эффективного хранения и запросов многомерных векторов. Они обеспечивают быстрый поиск по похожести, что критично для RAG-приложений. Популярные варианты: Chroma, Pinecone, Weaviate, FAISS.",
      "related": ["embedding", "similarity-search", "rag"],
      "example": "vectorstore = ChromaVectorStore.from_documents(docs, embeddings)"
    },
    "agent": {
      "term": "Агент",
      "full": "AI Agent (ИИ-агент)",
      "short": "LLM с возможностью использовать инструменты и выполнять действия",
      "long": "ИИ-агент — это LLM с возможностью использовать внешние инструменты (поиск, калькуляторы, API) и принимать решения о том, какие действия предпринять. Агенты могут разбивать сложные задачи на шаги и выполнять их автономно.",
      "related": ["tool", "react", "chain-of-thought"],
      "example": "agent = ReActAgent.from_openai('gpt-4o', tools=[search, calculator])"
    },
    "tool": {
      "term": "Инструмент",
      "full": "Agent Tool (Инструмент агента)",
      "short": "Функция, которую агент может вызывать для взаимодействия с внешними системами",
      "long": "Инструменты — это функции, которые агенты могут вызывать для выполнения конкретных действий: веб-поиск, запросы к базам данных, вычисления, вызовы API. Каждый инструмент имеет имя, описание и реализацию.",
      "related": ["agent", "function-calling"],
      "example": "@Tool(name='search', description='Поиск в интернете')\ndef search(query: str) -> str:\n    return results"
    },
    "prompt": {
      "term": "Промпт",
      "full": "Prompt / System Prompt (Промпт / Системный промпт)",
      "short": "Текстовая инструкция для LLM, направляющая её поведение",
      "long": "Промпт — это входной текст, который говорит LLM, что делать. Системные промпты задают общее поведение и личность, а пользовательские промпты содержат конкретный запрос. Качество промптов критично для получения хороших результатов.",
      "related": ["llm", "prompt-engineering", "template"],
      "example": "rlm.set_system_prompt('Ты полезный ассистент-программист.')"
    },
    "token": {
      "term": "Токен",
      "full": "Token (Токен)",
      "short": "Минимальная единица текста для LLM (примерно 4 символа)",
      "long": "Токены — базовые единицы, которые LLM используют для обработки текста. Один токен — примерно 4 символа или 0.75 слова на английском (на русском меньше). Цены LLM и лимиты контекста измеряются в токенах.",
      "related": ["context-window", "llm", "cost"],
      "example": "# 'Hello world' ≈ 2 токена\n# 1000 токенов ≈ 750 слов (английский)"
    },
    "context-window": {
      "term": "Контекстное окно",
      "full": "Context Window / Context Length (Окно контекста)",
      "short": "Максимальный объём текста, который LLM может обработать за раз",
      "long": "Контекстное окно — максимальное число токенов, которое LLM может обработать в одном запросе (ввод + вывод). GPT-4o имеет 128K токенов, Claude 3 — 200K. Больший контекст позволяет обрабатывать длинные документы, но стоит дороже.",
      "related": ["token", "infiniretri", "llm"],
      "example": "# GPT-4o: 128K токенов ≈ 96K слов\n# Claude 3: 200K токенов ≈ 150K слов"
    },
    "memory": {
      "term": "Память",
      "full": "Conversational Memory (Память разговора)",
      "short": "Система для хранения и извлечения контекста предыдущих сообщений",
      "long": "Память позволяет LLM помнить предыдущие сообщения в разговоре. Существуют разные типы: Buffer (хранит все сообщения), Summary (сжимает старые), Hierarchical (многоуровневое хранение). Хорошее управление памятью — ключ для чат-ботов.",
      "related": ["hmem", "buffer", "context-window"],
      "example": "memory = BufferMemory(max_messages=20)\nrlm.set_memory(memory)"
    },
    "infiniretri": {
      "term": "InfiniRetri",
      "full": "Infinite Retrieval (Бесконечное извлечение)",
      "short": "Уникальная техника RLM для работы с неограниченным контекстом",
      "long": "InfiniRetri — уникальный подход RLM-Toolkit для преодоления лимитов контекстного окна. Он динамически извлекает и инжектирует только самый релевантный контекст для каждого запроса, позволяя эффективно работать с документами, далеко превышающими нативный контекст модели.",
      "related": ["rag", "context-window", "retriever"],
      "example": "config = RLMConfig(enable_infiniretri=True)\nrlm = RLM.from_openai('gpt-4o', config=config)"
    },
    "hmem": {
      "term": "H-MEM",
      "full": "Hierarchical Memory (Иерархическая память)",
      "short": "Многоуровневая система памяти, вдохновлённая человеческим мозгом",
      "long": "H-MEM организует память в уровни: рабочая (непосредственный контекст), эпизодическая (конкретные события) и семантическая (дистиллированные знания). Это зеркалит человеческую память и обеспечивает эффективное долгосрочное управление контекстом.",
      "related": ["memory", "context-window", "agent"],
      "example": "memory = HierarchicalMemory()\nmemory.add_episode('Пользователь спросил о Python')"
    },
    "prompt-injection": {
      "term": "Prompt Injection",
      "full": "Prompt Injection Attack (Атака инъекции промпта)",
      "short": "Атака безопасности, где вредоносный ввод захватывает поведение LLM",
      "long": "Prompt injection — уязвимость безопасности, где атакующие создают ввод, который переопределяет или обходит инструкции системы. Примеры: атаки 'ignore previous instructions'. Защита требует валидации ввода и фильтрации вывода.",
      "related": ["security", "jailbreak", "guardrails"],
      "example": "detector = PromptInjectionDetector()\nresult = detector.detect(user_input)"
    }
  }
}
